{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Autoencoder for the multivariate time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from kan import KAN, KANLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Load and preprocess data from a CSV file\n",
    "def load_data(file_path):\n",
    "    # Assuming the CSV file has a header row and each row is a time series sample\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Custom PyTorch dataset class for time series data\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/mohammed/moment/data/Repressilator/One_Parameter/Repressilator_training.csv'\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Calculate mean and standard deviation for normalization\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = TimeSeriesDataset(data)\n",
    "data_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64, latent_dim),\n",
    "            nn.Tanh()\n",
    "        #    # add self attention layer\n",
    "        )\n",
    "\n",
    "        #self.encoder = KAN([input_dim, 10,10, latent_dim],grid_range=[-4,4],grid_size=10)\n",
    "        #self.decoder = KAN([latent_dim, 10,10, input_dim],grid_range=[-4,4],grid_size=10)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.LeakyReLU(),\n",
    "        #    #nn.Linear(32, 64),\n",
    "        #    #nn.LeakyReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "        # Parameters for normalization\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        \n",
    "    def normalize(self, x):\n",
    "\n",
    "        return (x - self.mean) / self.std\n",
    "    \n",
    "    def denormalize(self, x):\n",
    "        # Denormalize the data using precomputed mean and std\n",
    "        return x * self.std + self.mean\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalize input\n",
    "        x = self.normalize(x)\n",
    "        \n",
    "        # Encode and decode\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        \n",
    "        # Denormalize output\n",
    "        decoded = self.denormalize(decoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Normalize input\n",
    "        x = self.normalize(x)\n",
    "        \n",
    "        # Encode\n",
    "\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "    def decode(self, x):\n",
    "        # Decode\n",
    "        decoded = self.decoder(x)\n",
    "        # Denormalize output\n",
    "        decoded = self.denormalize(decoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = data.shape[1]   # Number of features in the multivariate time series\n",
    "latent_dim = 6              # Target dimension for the univariate representation\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the autoencoder\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "autoencoder = Autoencoder(input_dim=input_dim, latent_dim=latent_dim)\n",
    "\n",
    "# Set normalization parameters\n",
    "autoencoder.mean = torch.tensor(mean.values, dtype=torch.float32)\n",
    "autoencoder.std = torch.tensor(std.values, dtype=torch.float32)\n",
    "\n",
    "# Check if GPU is available and move the model and tensors to GPU\n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "autoencoder.mean = autoencoder.mean.to(device)\n",
    "autoencoder.std = autoencoder.std.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 17\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss1 \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_loader)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#if (epoch + 1) % 10 == 0:\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/optim/optimizer.py:338\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_compiling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    339\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_current_stream_capturing()\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups):\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/_utils.py:857\u001b[0m, in \u001b[0;36mis_compiling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_compiling\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    852\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m    Indicates whether we are tracing/compiling with torch.compile() or torch.export().\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \n\u001b[1;32m    855\u001b[0m \u001b[38;5;124;03m    TODO(khabinov): we should deprecate this function and use torch.compiler.is_compiling().\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_compiling\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/compiler/__init__.py:173\u001b[0m, in \u001b[0;36mis_compiling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_compiling\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m        >>>     ...rest of the function...\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_scripting\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/moment/lib/python3.11/site-packages/torch/_jit_internal.py:1120\u001b[0m, in \u001b[0;36mis_scripting\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28mglobals\u001b[39m()[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcastingList\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m BroadcastingList1\n\u001b[0;32m-> 1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_scripting\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;124;03m    Function that returns True when in compilation and False otherwise. This\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m    is useful especially with the @unused decorator to leave code in your\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;124;03m              return unsupported_linear_op(x)\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss1 = 0\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)  # Move data to GPU\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed = autoencoder(batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        loss1 = loss1 + loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss1 = loss1 / len(data_loader)\n",
    "    #if (epoch + 1) % 10 == 0:\n",
    "    #print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss1: {loss1.item():.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(autoencoder.state_dict(), 'autoencoder_model.pth')\n",
    "\n",
    "# Optional: Evaluate the model on test data\n",
    "def evaluate_model(autoencoder, data_loader):\n",
    "    autoencoder.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)  # Move data to GPU\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            total_loss += loss.item()\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "    return average_loss\n",
    "\n",
    "# Evaluate on training data (replace with test data if available)\n",
    "train_loss = evaluate_model(autoencoder, data_loader)\n",
    "print(f'Training Data Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model from autoencoder_model.pth\n",
    "\n",
    "autoencoder.load_state_dict(torch.load('autoencoder_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import momentfm\n",
    "from momentfm import MOMENTPipeline\n",
    "\n",
    "model = MOMENTPipeline.from_pretrained(\n",
    "    \"AutonLab/MOMENT-1-large\",\n",
    "    model_kwargs={\n",
    "        'task_name': 'forecasting',\n",
    "        'forecast_horizon': 192,\n",
    "        'head_dropout': 0.1,\n",
    "        'weight_decay': 0,\n",
    "        'freeze_encoder': True, # Freeze the patch embedding layer\n",
    "        'freeze_embedder': True, # Freeze the transformer encoder\n",
    "        'freeze_head': False, # The linear forecasting head must be trained\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOMENTPipeline(\n",
      "  (normalizer): RevIN()\n",
      "  (tokenizer): Patching()\n",
      "  (patch_embedding): PatchEmbedding(\n",
      "    (value_embedding): Linear(in_features=8, out_features=1024, bias=False)\n",
      "    (position_embedding): PositionalEmbedding()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 1024)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 16)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-23): 23 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseGatedActDense(\n",
      "              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): NewGELUActivation()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (head): ForecastingHead(\n",
      "    (flatten): Flatten(start_dim=-2, end_dim=-1)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear): Linear(in_features=65536, out_features=192, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.init()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfrozen parameters:\n",
      "     head.linear.weight\n",
      "     head.linear.bias\n"
     ]
    }
   ],
   "source": [
    "print(\"Unfrozen parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print('    ', name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torch.cuda.amp\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "from momentfm.utils.utils import control_randomness\n",
    "#from momentfm.data.informer_dataset import InformerDataset\n",
    "from momentfm.utils.forecasting_metrics import get_forecasting_metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Iformer dataset using 192 forecasting horizon and 4 timeseries for each 1024 consisting of 512 context and 192 forecasting\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class InformerDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecast_horizon: Optional[int] = 192,\n",
    "        data_split: str = \"train\",\n",
    "        data_stride_len: int = 80,\n",
    "        task_name: str = \"forecasting\",\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        forecast_horizon : int\n",
    "            Length of the prediction sequence.\n",
    "        data_split : str\n",
    "            Split of the dataset, 'train' or 'test'.\n",
    "        data_stride_len : int\n",
    "            Stride length when generating consecutive\n",
    "            time series windows.\n",
    "        task_name : str\n",
    "            The task that the dataset is used for. One of\n",
    "            'forecasting', or  'imputation'.\n",
    "        random_seed : int\n",
    "            Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        self.seq_len = 512\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.full_file_path_and_name = \"/home/mohammed/moment/data/Repressilator/One_Parameter/Repressilator_training.csv\"\n",
    "        self.validation_data =\"/home/mohammed/moment/data/Repressilator/One_Parameter/Repressilator_validation.csv\"\n",
    "        self.data_split = data_split\n",
    "        self.data_stride_len = data_stride_len\n",
    "        self.task_name = task_name\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Read data\n",
    "        self._read_data()\n",
    "\n",
    "    def _read_data(self):\n",
    "        self.scale = StandardScaler()\n",
    "        if self.data_split == \"train\":\n",
    "            df = pd.read_csv(self.full_file_path_and_name)\n",
    "        elif self.data_split == \"test\":\n",
    "            df = pd.read_csv(self.validation_data)\n",
    "        self.length_timeseries_original = df.shape[0]\n",
    "        self.n_channels = df.shape[1] - 1\n",
    "\n",
    "        df = df.infer_objects(copy=False).interpolate(method=\"cubic\")\n",
    "        self.data = df.values\n",
    "\n",
    "        self.length_timeseries = self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = (index // 4)* 1024\n",
    "        y = (index % 4)* self.data_stride_len\n",
    "        seq_start = x+y\n",
    "        seq_end = seq_start + self.seq_len\n",
    "        input_mask = np.ones(self.seq_len)\n",
    "\n",
    "        if self.task_name == \"forecasting\":\n",
    "            pred_end = seq_end + self.forecast_horizon\n",
    "\n",
    "            if pred_end > self.length_timeseries:\n",
    "                pred_end = self.length_timeseries\n",
    "                #seq_end = seq_end - self.forecast_horizon\n",
    "                seq_end = self.length_timeseries - self.forecast_horizon\n",
    "                seq_start = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "            forecast = self.data[seq_end:pred_end, :].T\n",
    "\n",
    "            return timeseries, forecast, input_mask\n",
    "\n",
    "        elif self.task_name == \"imputation\":\n",
    "            if seq_end > self.length_timeseries:\n",
    "                seq_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "\n",
    "            return timeseries, input_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.task_name == \"imputation\":\n",
    "            return (self.length_timeseries - self.seq_len) // self.data_stride_len + 1\n",
    "        elif self.task_name == \"forecasting\":\n",
    "            return (\n",
    "                self.length_timeseries - self.seq_len - self.forecast_horizon\n",
    "            ) // self.data_stride_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is informer dataset for 512 context and 512 forecasting horizon \n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class InformerDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        forecast_horizon: Optional[int] = 192,\n",
    "        data_split: str = \"train\",\n",
    "        data_stride_len: int = 1024,\n",
    "        task_name: str = \"forecasting\",\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        forecast_horizon : int\n",
    "            Length of the prediction sequence.\n",
    "        data_split : str\n",
    "            Split of the dataset, 'train' or 'test'.\n",
    "        data_stride_len : int\n",
    "            Stride length when generating consecutive\n",
    "            time series windows.\n",
    "        task_name : str\n",
    "            The task that the dataset is used for. One of\n",
    "            'forecasting', or  'imputation'.\n",
    "        random_seed : int\n",
    "            Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "\n",
    "        self.seq_len = 512\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.full_file_path_and_name = \"/home/mohammed/moment/data/Repressilator/One_Parameter/Repressilator_training.csv\"\n",
    "        self.validation_data =\"/home/mohammed/moment/data/Repressilator/One_Parameter/Repressilator_validation.csv\"\n",
    "        self.data_split = data_split\n",
    "        self.data_stride_len = data_stride_len\n",
    "        self.task_name = task_name\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "        # Read data\n",
    "        self._read_data()\n",
    "\n",
    "    def _read_data(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        if self.data_split == \"train\":\n",
    "            df = pd.read_csv(self.full_file_path_and_name)\n",
    "            print(df.shape)\n",
    "        elif self.data_split == \"test\":\n",
    "            df = pd.read_csv(self.validation_data)\n",
    "        self.length_timeseries_original = df.shape[0]\n",
    "        self.n_channels = df.shape[1] - 1\n",
    "\n",
    "        df = df.infer_objects(copy=False).interpolate(method=\"cubic\")\n",
    "        self.data = df.values\n",
    "\n",
    "\n",
    "        #train_data = df\n",
    "        #self.scaler.fit(train_data.values)\n",
    "        #df = self.scaler.transform(df.values)\n",
    "        #self.data = df\n",
    "        #self.data = df.values\n",
    "\n",
    "        self.length_timeseries = self.data.shape[0]\n",
    "        #print(f'the length of the time series is {self.length_timeseries}')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = index * 1024\n",
    "        #y = index* self.data_stride_len\n",
    "        #seq_start = x+y\n",
    "        seq_start = x\n",
    "        seq_end = seq_start + self.seq_len\n",
    "        input_mask = np.ones(self.seq_len)\n",
    "\n",
    "        if self.task_name == \"forecasting\":\n",
    "            pred_end = seq_end + self.forecast_horizon\n",
    "\n",
    "            if pred_end > self.length_timeseries:\n",
    "                pred_end = self.length_timeseries\n",
    "                #seq_end = seq_end - self.forecast_horizon\n",
    "                seq_end = self.length_timeseries - self.forecast_horizon\n",
    "                seq_start = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "            forecast = self.data[seq_end:pred_end, :].T\n",
    "\n",
    "            return timeseries, forecast, input_mask\n",
    "\n",
    "        elif self.task_name == \"imputation\":\n",
    "            if seq_end > self.length_timeseries:\n",
    "                seq_end = self.length_timeseries\n",
    "                seq_end = seq_end - self.seq_len\n",
    "\n",
    "            timeseries = self.data[seq_start:seq_end, :].T\n",
    "\n",
    "            return timeseries, input_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.task_name == \"imputation\":\n",
    "            return (self.length_timeseries - self.seq_len) // self.data_stride_len + 1\n",
    "        elif self.task_name == \"forecasting\":\n",
    "            return (\n",
    "                self.length_timeseries - self.seq_len - self.forecast_horizon\n",
    "            ) // self.data_stride_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for PyTorch, Numpy etc.\n",
    "control_randomness(seed=13)\n",
    "\n",
    "# Load data\n",
    "train_dataset = InformerDataset(data_split=\"train\", random_seed=13, forecast_horizon=192)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#mu = train_dataset.scale.mean_\n",
    "#std = train_dataset.scale.scale_\n",
    "# make the mean and the standard deviation to be in device\n",
    "#mu = torch.tensor(mu, dtype=torch.float32).to(device)\n",
    "#std = torch.tensor(std, dtype=torch.float32).to(device)\n",
    "\n",
    "test_dataset = InformerDataset(data_split=\"test\", random_seed=13, forecast_horizon=192)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:12<00:00, 24.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss: 0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Test MSE: 12.712 | Test MAE: 2.190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3276 [00:00<?, ?it/s]/home/mohammed/miniconda3/envs/moment/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/mohammed/miniconda3/envs/moment/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "100%|██████████| 3276/3276 [02:20<00:00, 23.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Test MSE: 7.335 | Test MAE: 1.667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:22<00:00, 23.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Test MSE: 6.227 | Test MAE: 1.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:19<00:00, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Test MSE: 5.981 | Test MAE: 1.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:19<00:00, 23.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Test MSE: 6.088 | Test MAE: 1.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:19<00:00, 23.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Test MSE: 5.736 | Test MAE: 1.610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:28<00:00, 22.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Test MSE: 5.182 | Test MAE: 1.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:23<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Test MSE: 5.717 | Test MAE: 1.655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:20<00:00, 23.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:04<00:00, 25.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Test MSE: 5.505 | Test MAE: 1.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3276/3276 [02:19<00:00, 23.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train loss: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:03<00:00, 25.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Test MSE: 5.422 | Test MAE: 1.612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "autoencoder.to(device)\n",
    "cur_epoch = 0\n",
    "max_epoch = 10\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Move the loss function to the GPU\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Create a OneCycleLR scheduler\n",
    "max_lr = 1e-4\n",
    "total_steps = len(train_loader) * max_epoch\n",
    "scheduler = OneCycleLR(optimizer, max_lr=max_lr, total_steps=total_steps, pct_start=0.3)\n",
    "\n",
    "# Gradient clipping value\n",
    "max_norm = 5.0\n",
    "\n",
    "\n",
    "autoencoder.eval()\n",
    "while cur_epoch < max_epoch:\n",
    "    losses = []\n",
    "    for timeseries, forecast, input_mask in tqdm(train_loader, total=len(train_loader)):\n",
    "        # Move the data to the GPU\n",
    "        timeseries = timeseries.float().to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        forecast = forecast.float().to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            timeseries = timeseries.permute(0, 2, 1)\n",
    "            timeseries = autoencoder.encode(timeseries)\n",
    "            timeseries = timeseries.permute(0, 2, 1)\n",
    "            forecast = forecast.permute(0, 2, 1)\n",
    "            forecast = autoencoder.encode(forecast)\n",
    "            forecast = forecast.permute(0, 2, 1)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # output the forecast\n",
    "\n",
    "            #timeseries = timeseries.permute(0, 2, 1)\n",
    "            #timeseries = autoencoder.encode(timeseries)\n",
    "                # change the shape of the encoded from torch.Size([8, 512, 1]) to torch.Size([8, 1, 512])\n",
    "            #timeseries = timeseries.permute(0, 2, 1)\n",
    "            output = model(timeseries, input_mask)\n",
    "            #output.forecast = output.forecast.permute(0, 2, 1)\n",
    "            #output.forecast = autoencoder.decode(output.forecast)\n",
    "            #output.forecast = output.forecast.permute(0, 2, 1)\n",
    "\n",
    "        loss = criterion(output.forecast, forecast)\n",
    "        # Scales the loss for mixed precision training\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    print(f\"Epoch {cur_epoch}: Train loss: {average_loss:.3f}\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    cur_epoch += 1\n",
    "\n",
    "    # Evaluate the model on the test split\n",
    "    trues, preds, histories, losses = [], [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for timeseries, forecast, input_mask in tqdm(test_loader, total=len(test_loader)):\n",
    "        # Move the data to the GPU\n",
    "            timeseries = timeseries.float().to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            forecast = forecast.float().to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # encode the timeseries\n",
    "                # change the shape of the timeseries from torch.Size([8, 6, 512]) to torch.Size([8, 512, 6])\n",
    "                timeseries = timeseries.permute(0, 2, 1).to(device)\n",
    "                #timeseries = (timeseries - mu)/std\n",
    "                encoded = autoencoder.encode(timeseries)\n",
    "                # change the shape of the encoded from torch.Size([8, 512, 1]) to torch.Size([8, 1, 512])\n",
    "                encoded = encoded.permute(0, 2, 1).to(device)\n",
    "\n",
    "                # output the forecast\n",
    "                output = model(encoded, input_mask)\n",
    "                # change the shape of the forecast from torch.Size([8, 1, 192]) to torch.Size([8, 192, 1])\n",
    "                output.forecast = output.forecast.permute(0, 2, 1).to(device)\n",
    "                # decode the forecast\n",
    "                output.forecast = autoencoder.decode(output.forecast)\n",
    "                #output.forecast = output.forecast * std + mu\n",
    "                output.forecast = output.forecast.permute(0, 2, 1).to(device)\n",
    "\n",
    "            loss = criterion(output.forecast, forecast)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            trues.append(forecast.detach().cpu().numpy())\n",
    "            preds.append(output.forecast.detach().cpu().numpy())\n",
    "            histories.append(timeseries.detach().cpu().numpy())\n",
    "\n",
    "    losses = np.array(losses)\n",
    "    average_loss = np.average(losses)\n",
    "    model.train()\n",
    "\n",
    "    trues = np.concatenate(trues, axis=0)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    histories = np.concatenate(histories, axis=0)\n",
    "\n",
    "    metrics = get_forecasting_metrics(y=trues, y_hat=preds, reduction='mean')\n",
    "\n",
    "    print(f\"Epoch {cur_epoch}: Test MSE: {metrics.mse:.3f} | Test MAE: {metrics.mae:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
